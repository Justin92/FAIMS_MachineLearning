{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Simple XGBoost Bayesian Hyperparam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import xgboost\n",
    "import hyperopt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from sklearn import datasets, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a multilabel classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "X1, y1 = make_classification(n_features=50, n_samples=1000)\n",
    "X2, y2 = make_classification(n_features=50, n_samples=1000)\n",
    "X = np.concatenate((X1, X2), axis=1)\n",
    "y = np.transpose([y2, y1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923685241428956"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making sure I can use cross validation score with multilabel\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "myXGBC = OneVsRestClassifier(XGBClassifier(max_depth=3, subsample=0.5), n_jobs=2)\n",
    "mean_score = cross_val_score(myXGBC, X, y, cv=5, scoring='roc_auc_ovr_weighted').mean()\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameter space for hyperopt\n",
    "xgbc_param_hyperopt= {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),          #1\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 5, 15, 1)),                        #3\n",
    "    'max_delta_step': scope.int(hp.quniform('max_delta_step', 0, 10, 1)),              #2\n",
    "    'booster': hp.choice('booster', ['gbtree', 'dart']),                               #0\n",
    "    'subsample': hp.uniform('subsample', 0.0, 1.0),                                    #7\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 150),                        #6\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'hist']),                         #8\n",
    "    'sampling_method': hp.choice('sampling_method', ['uniform', 'gradient_based']),    #5\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)                                   #4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperopt function\n",
    "def hyperopt(param_space, X_train, y_train, X_test, y_test, num_eval):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    def objective_function(params):\n",
    "        clf = OneVsRestClassifier(XGBClassifier(**params), n_jobs=2)\n",
    "        score = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc_ovr_weighted').mean()\n",
    "        return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective_function, \n",
    "                      param_space, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate= np.random.RandomState(23))\n",
    "    loss = [x['result']['loss'] for x in trials.trials]\n",
    "    \n",
    "    best_param_values = [x for x in best_param.values()]\n",
    "    \n",
    "    if best_param_values[0] == 0:\n",
    "        booster = 'gbtree'\n",
    "    else:\n",
    "        booster = 'dart'\n",
    "        \n",
    "        \n",
    "    if best_param_values[8] == 0:\n",
    "        tree_method = 'auto'\n",
    "    else:\n",
    "        tree_method = 'hist'\n",
    "        \n",
    "    \n",
    "    if best_param_values[5] == 0:\n",
    "        sampling_method = 'uniform'\n",
    "    else:\n",
    "        sampling_method = 'gradient_based'\n",
    "    \n",
    "    clf_best = OneVsRestClassifier(XGBClassifier(learning_rate=best_param_values[1],\n",
    "                                  max_depth=int(best_param_values[3]),\n",
    "                                  max_delta_step=int(best_param_values[2]),\n",
    "                                  booster=booster,\n",
    "                                  subsample=best_param_values[7],\n",
    "                                  scale_pos_weight = best_param_values[6],\n",
    "                                  tree_method = tree_method,\n",
    "                                  sampling_method = sampling_method,\n",
    "                                  reg_lambda = best_param_values[4],\n",
    "                                    \n",
    "                                                 \n",
    "                                ), n_jobs=2)\n",
    "                                  \n",
    "    clf_best.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"##### Results\")\n",
    "    print(\"Score best parameters: \", min(loss)*-1)\n",
    "    print(\"Best parameters: \", best_param)\n",
    "    print(\"Test Score: \", clf_best.score(X_test, y_test))\n",
    "    print(\"Time elapsed: \", time.time() - start)\n",
    "    print(\"Parameter combinations evaluated: \", num_eval)\n",
    "    \n",
    "    return trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:58<00:00,  7.95s/trial, best loss: -0.9922728076435889]\n",
      "\n",
      "##### Results\n",
      "Score best parameters:  0.9922728076435889\n",
      "Best parameters:  {'booster': 1, 'learning_rate': 0.06393618887304636, 'max_delta_step': 3.0, 'max_depth': 10.0, 'reg_lambda': 0.8009949659818361, 'sampling_method': 0, 'scale_pos_weight': 1.0618494241443832, 'subsample': 0.4605314441128805, 'tree_method': 0}\n",
      "Test Score:  0.935\n",
      "Time elapsed:  239.73936104774475\n",
      "Parameter combinations evaluated:  30\n"
     ]
    }
   ],
   "source": [
    "results_hyperopt = hyperopt(xgbc_param_hyperopt, X_train, y_train, X_test, y_test, num_eval=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
